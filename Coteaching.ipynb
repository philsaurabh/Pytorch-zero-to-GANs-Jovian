{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Node Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philsaurabh/Pytorch-zero-to-GANs-Jovian/blob/main/Coteaching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1op-CbyLuN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7a508c-7c08-46ca-ce42-6bf6a96e955d"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import numpy as np\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "import torch\n",
        "def build_uniform_P(size, noise):\n",
        "    \"\"\" The noise matrix flips any class to any other with probability\n",
        "    noise / (#class - 1).\n",
        "    \"\"\"\n",
        "\n",
        "    assert(noise >= 0.) and (noise <= 1.)\n",
        "\n",
        "    P = np.float64(noise) / np.float64(size - 1) * np.ones((size, size))\n",
        "    np.fill_diagonal(P, (np.float64(1)-np.float64(noise))*np.ones(size))\n",
        "    \n",
        "    diag_idx = np.arange(size)\n",
        "    P[diag_idx,diag_idx] = P[diag_idx,diag_idx] + 1.0 - P.sum(0)\n",
        "    assert_array_almost_equal(P.sum(axis=1), 1, 1)\n",
        "    return P\n",
        "\n",
        "def build_pair_p(size, noise):\n",
        "    assert(noise >= 0.) and (noise <= 1.)\n",
        "    P = (1.0 - np.float64(noise)) * np.eye(size)\n",
        "    for i in range(size):\n",
        "        P[i,i-1] = np.float64(noise)\n",
        "    assert_array_almost_equal(P.sum(axis=1), 1, 1)\n",
        "    return P\n",
        "\n",
        "\n",
        "def multiclass_noisify(y, P, random_state=0):\n",
        "    \"\"\" Flip classes according to transition probability matrix T.\n",
        "    It expects a number between 0 and the number of classes - 1.\n",
        "    \"\"\"\n",
        "\n",
        "    assert P.shape[0] == P.shape[1]\n",
        "    assert np.max(y) < P.shape[0]\n",
        "\n",
        "    # row stochastic matrix\n",
        "    assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
        "    assert (P >= 0.0).all()\n",
        "\n",
        "    m = y.shape[0]\n",
        "    new_y = y.copy()\n",
        "    flipper = np.random.RandomState(random_state)\n",
        "\n",
        "    for idx in np.arange(m):\n",
        "        i = y[idx]\n",
        "        # draw a vector with only an 1\n",
        "        flipped = flipper.multinomial(1, P[i, :], 1)[0]\n",
        "        new_y[idx] = np.where(flipped == 1)[0]\n",
        "\n",
        "    return new_y\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    sparserow=torch.LongTensor(sparse_mx.row).unsqueeze(1)\n",
        "    sparsecol=torch.LongTensor(sparse_mx.col).unsqueeze(1)\n",
        "    sparseconcat=torch.cat((sparserow, sparsecol),1)\n",
        "    sparsedata=torch.FloatTensor(sparse_mx.data)\n",
        "    return torch.sparse.FloatTensor(sparseconcat.t(),sparsedata,torch.Size(sparse_mx.shape))\n",
        "\n",
        "def noisify(y, p_minus, p_plus=None, random_state=0):\n",
        "    \"\"\" Flip labels with probability p_minus.\n",
        "    If p_plus is given too, the function flips with asymmetric probability.\n",
        "    \"\"\"\n",
        "\n",
        "    assert np.all(np.abs(y) == 1)\n",
        "\n",
        "    m = y.shape[0]\n",
        "    new_y = y.copy()\n",
        "    coin = np.random.RandomState(random_state)\n",
        "\n",
        "    if p_plus is None:\n",
        "        p_plus = p_minus\n",
        "\n",
        "    # This can be made much faster by tossing all the coins and completely\n",
        "    # avoiding the loop. Although, it is not simple to write the asymmetric\n",
        "    # case then.\n",
        "    for idx in np.arange(m):\n",
        "        if y[idx] == -1:\n",
        "            if coin.binomial(n=1, p=p_minus, size=1) == 1:\n",
        "                new_y[idx] = -new_y[idx]\n",
        "        else:\n",
        "            if coin.binomial(n=1, p=p_plus, size=1) == 1:\n",
        "                new_y[idx] = -new_y[idx]\n",
        "\n",
        "    return new_y\n",
        "\n",
        "def noisify_with_P(y_train, nb_classes, noise, random_state=None,  noise_type='uniform'):\n",
        "\n",
        "    if noise > 0.0:\n",
        "        if noise_type=='uniform':\n",
        "            print('Uniform noise')\n",
        "            P = build_uniform_P(nb_classes, noise)\n",
        "        elif noise_type == 'pair':\n",
        "            print('Pair noise')\n",
        "            P = build_pair_p(nb_classes, noise)\n",
        "        else:\n",
        "            print('Noise type have implemented')\n",
        "        # seed the random numbers with #run\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "    else:\n",
        "        P = np.eye(nb_classes)\n",
        "\n",
        "    return y_train, P\n",
        "\n",
        "def to_onehot(labels):\n",
        "    class_size = labels.max() + 1\n",
        "    onehot = np.eye(class_size)\n",
        "    \n",
        "    return onehot[labels]\n",
        "\n",
        "# %%\n",
        "import os\n",
        "def load_emd(path, dataset):\n",
        "\n",
        "    graph_embedding = np.genfromtxt(\n",
        "            os.path.join(path,\"{}.emb\".format(dataset)),\n",
        "            skip_header=1,\n",
        "            dtype=float)\n",
        "    embedding = np.zeros([graph_embedding.shape[0],graph_embedding.shape[1]-1])\n",
        "\n",
        "    for i in range(graph_embedding.shape[0]):\n",
        "        embedding[int(graph_embedding[i,0])] = graph_embedding[i,1:]\n",
        "    \n",
        "    return embedding\n",
        "# %%\n",
        "from sklearn.model_selection import train_test_split\n",
        "def get_train_val_test(nnodes, val_size=0.1, test_size=0.8, stratify=None, seed=None):\n",
        "    \"\"\"This setting follows nettack/mettack, where we split the nodes\n",
        "    into 10% training, 10% validation and 80% testing data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    nnodes : int\n",
        "        number of nodes in total\n",
        "    val_size : float\n",
        "        size of validation set\n",
        "    test_size : float\n",
        "        size of test set\n",
        "    stratify :\n",
        "        data is expected to split in a stratified fashion. So stratify should be labels.\n",
        "    seed : int or None\n",
        "        random seed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    idx_train :\n",
        "        node training indices\n",
        "    idx_val :\n",
        "        node validation indices\n",
        "    idx_test :\n",
        "        node test indices\n",
        "    \"\"\"\n",
        "\n",
        "    assert stratify is not None, 'stratify cannot be None!'\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    idx = np.arange(nnodes)\n",
        "    train_size = 1 - val_size - test_size\n",
        "    idx_train_and_val, idx_test = train_test_split(idx,\n",
        "                                                   random_state=None,\n",
        "                                                   train_size=train_size + val_size,\n",
        "                                                   test_size=test_size,\n",
        "                                                   stratify=stratify)\n",
        "\n",
        "    if stratify is not None:\n",
        "        stratify = stratify[idx_train_and_val]\n",
        "\n",
        "    idx_train, idx_val = train_test_split(idx_train_and_val,\n",
        "                                          random_state=None,\n",
        "                                          train_size=(train_size / (train_size + val_size)),\n",
        "                                          test_size=(val_size / (train_size + val_size)),\n",
        "                                          stratify=stratify)\n",
        "\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    \"\"\"Return accuracy of output compared to labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output : torch.Tensor\n",
        "        output from model\n",
        "    labels : torch.Tensor or numpy.array\n",
        "        node labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        accuracy\n",
        "    \"\"\"\n",
        "    if not hasattr(labels, '__len__'):\n",
        "        labels = [labels]\n",
        "    if type(labels) is not torch.Tensor:\n",
        "        labels = torch.LongTensor(labels)\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ],
      "metadata": {
        "id": "dhVBTmuDrsSj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import os.path as osp\n",
        "import os\n",
        "import urllib.request\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "\n",
        "class Dataset():\n",
        "    \"\"\"Dataset class contains four citation network datasets \"cora\", \"cora-ml\", \"citeseer\" and \"pubmed\",\n",
        "    and one blog dataset \"Polblogs\".\n",
        "    The 'cora', 'cora-ml', 'poblogs' and 'citeseer' are downloaded from https://github.com/danielzuegner/gnn-meta-attack/tree/master/data, and 'pubmed' is from https://github.com/tkipf/gcn/tree/master/gcn/data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root :\n",
        "        root directory where the dataset should be saved.\n",
        "    name :\n",
        "        dataset name, it can be choosen from ['cora', 'citeseer', 'cora_ml', 'polblogs', 'pubmed']\n",
        "    seed :\n",
        "        random seed for splitting training/validation/test.\n",
        "    --------\n",
        "\tWe can first create an instance of the Dataset class and then take out its attributes.\n",
        "\n",
        "\t>>> from deeprobust.graph.data import Dataset\n",
        "\t>>> data = Dataset(root='/tmp/', name='cora')\n",
        "\t>>> adj, features, labels = data.adj, data.features, data.labels\n",
        "\t>>> idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, name, seed=None):\n",
        "        self.name = name.lower()\n",
        "\n",
        "        assert self.name in ['cora', 'citeseer', 'cora_ml', 'polblogs', 'pubmed'], \\\n",
        "            'Currently only support cora, citeseer, cora_ml, polblogs, pubmed'\n",
        "\n",
        "        self.seed = seed\n",
        "        \n",
        "        self.url =  'https://raw.githubusercontent.com/danielzuegner/gnn-meta-attack/master/data/%s.npz' % self.name\n",
        "        self.root = osp.expanduser(osp.normpath(root))\n",
        "        self.data_folder = osp.join(root, self.name)\n",
        "        self.data_filename = self.data_folder + '.npz'\n",
        "\n",
        "        self.adj, self.features, self.labels = self.load_data()\n",
        "        self.idx_train, self.idx_val, self.idx_test = self.get_train_val_test()\n",
        "\n",
        "    def get_train_val_test(self):\n",
        "        \"\"\"Get training, validation, test splits\n",
        "        \"\"\"\n",
        "        return get_train_val_test(nnodes=self.adj.shape[0], val_size=0.1, test_size=0.8, stratify=self.labels, seed=self.seed)\n",
        "\n",
        "    def load_data(self):\n",
        "        print('Loading {} dataset...'.format(self.name))\n",
        "        if self.name == 'pubmed':\n",
        "            return self.load_pubmed()\n",
        "\n",
        "        if not osp.exists(self.data_filename):\n",
        "            self.download_npz()\n",
        "\n",
        "        adj, features, labels = self.get_adj()\n",
        "        return adj, features, labels\n",
        "\n",
        "    def download_npz(self):\n",
        "        \"\"\"Download adjacen matrix npz file from self.url.\n",
        "        \"\"\"\n",
        "        print('Dowloading from {} to {}'.format(self.url, self.data_filename))\n",
        "        try:\n",
        "            urllib.request.urlretrieve(self.url, self.data_filename)\n",
        "        except:\n",
        "            raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n",
        "\n",
        "    def download_pubmed(self, name):\n",
        "        url = 'https://raw.githubusercontent.com/tkipf/gcn/master/gcn/data/'\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url + name, osp.join(self.root, name))\n",
        "        except:\n",
        "            raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n",
        "\n",
        "\n",
        "    def load_pubmed(self):\n",
        "        dataset = 'pubmed'\n",
        "        names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "        objects = []\n",
        "        for i in range(len(names)):\n",
        "            name = \"ind.{}.{}\".format(dataset, names[i])\n",
        "            data_filename = osp.join(self.root, name)\n",
        "\n",
        "            if not osp.exists(data_filename):\n",
        "                self.download_pubmed(name)\n",
        "\n",
        "            with open(data_filename, 'rb') as f:\n",
        "                if sys.version_info > (3, 0):\n",
        "                    objects.append(pkl.load(f, encoding='latin1'))\n",
        "                else:\n",
        "                    objects.append(pkl.load(f))\n",
        "\n",
        "        x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "\n",
        "\n",
        "        test_idx_file = \"ind.{}.test.index\".format(dataset)\n",
        "        if not osp.exists(osp.join(self.root, test_idx_file)):\n",
        "            self.download_pubmed(test_idx_file)\n",
        "\n",
        "        test_idx_reorder = parse_index_file(osp.join(self.root, test_idx_file))\n",
        "        test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "        features = sp.vstack((allx, tx)).tolil()\n",
        "        features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "        adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "        labels = np.vstack((ally, ty))\n",
        "        labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "        labels = np.where(labels)[1]\n",
        "        return adj, features, labels\n",
        "\n",
        "    def get_adj(self):\n",
        "        adj, features, labels = self.load_npz(self.data_filename)\n",
        "        adj = adj + adj.T\n",
        "        adj = adj.tolil()\n",
        "        adj[adj > 1] = 1\n",
        "\n",
        "        lcc = self.largest_connected_components(adj)\n",
        "        adj = adj[lcc][:, lcc]\n",
        "        features = features[lcc]\n",
        "        labels = labels[lcc]\n",
        "        assert adj.sum(0).A1.min() > 0, \"Graph contains singleton nodes\"\n",
        "\n",
        "        # whether to set diag=0?\n",
        "        adj.setdiag(0)\n",
        "        adj = adj.astype(\"float32\").tocsr()\n",
        "        adj.eliminate_zeros()\n",
        "\n",
        "        assert np.abs(adj - adj.T).sum() == 0, \"Input graph is not symmetric\"\n",
        "        assert adj.max() == 1 and len(np.unique(adj[adj.nonzero()].A1)) == 1, \"Graph must be unweighted\"\n",
        "\n",
        "        return adj, features, labels\n",
        "\n",
        "    def load_npz(self, file_name, is_sparse=True):\n",
        "        with np.load(file_name) as loader:\n",
        "            # loader = dict(loader)\n",
        "            if is_sparse:\n",
        "                adj = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
        "                                            loader['adj_indptr']), shape=loader['adj_shape'])\n",
        "                if 'attr_data' in loader:\n",
        "                    features = sp.csr_matrix((loader['attr_data'], loader['attr_indices'],\n",
        "                                                 loader['attr_indptr']), shape=loader['attr_shape'])\n",
        "                else:\n",
        "                    features = None\n",
        "                labels = loader.get('labels')\n",
        "            else:\n",
        "                adj = loader['adj_data']\n",
        "                if 'attr_data' in loader:\n",
        "                    features = loader['attr_data']\n",
        "                else:\n",
        "                    features = None\n",
        "                labels = loader.get('labels')\n",
        "        if features is None:\n",
        "            features = np.eye(adj.shape[0])\n",
        "        features = sp.csr_matrix(features, dtype=np.float32)\n",
        "        return adj, features, labels\n",
        "\n",
        "    def largest_connected_components(self, adj, n_components=1):\n",
        "        \"\"\"Select k largest connected components.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tadj : scipy.sparse.csr_matrix\n",
        "\t\t\tinput adjacency matrix\n",
        "\t\tn_components : int\n",
        "\t\t\tn largest connected components we want to select\n",
        "\t\t\"\"\"\n",
        "\n",
        "        _, component_indices = sp.csgraph.connected_components(adj)\n",
        "        component_sizes = np.bincount(component_indices)\n",
        "        components_to_keep = np.argsort(component_sizes)[::-1][:n_components]  # reverse order to sort descending\n",
        "        nodes_to_keep = [\n",
        "            idx for (idx, component) in enumerate(component_indices) if component in components_to_keep]\n",
        "        print(\"Selecting {0} largest connected components\".format(n_components))\n",
        "        return nodes_to_keep\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{0}(adj_shape={1}, feature_shape={2})'.format(self.name, self.adj.shape, self.features.shape)\n",
        "\n",
        "    def onehot(self, labels):\n",
        "        eye = np.identity(labels.max() + 1)\n",
        "        onehot_mx = eye[labels]\n",
        "        return onehot_mx\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "G0bDCzRcrwNu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q deeprobust"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cRwbP42sO3-",
        "outputId": "19b435d2-13fa-4dc8-c3d7-8fe59d3ca84a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeprobust\n",
            "  Downloading deeprobust-0.2.4-py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.12.1+cu113)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.56.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.0.2)\n",
            "Collecting tensorboardX>=2.0\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 31.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.18.3)\n",
            "Collecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.7.3)\n",
            "Requirement already satisfied: tqdm>=3.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (4.64.0)\n",
            "Collecting gensim<4.0,>=3.8\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (2.6.3)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.13.1+cu113)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8->deeprobust) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8->deeprobust) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->deeprobust) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (0.39.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (4.12.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->deeprobust) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->deeprobust) (3.1.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.0->deeprobust) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deeprobust) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.48.0->deeprobust) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (1.24.3)\n",
            "Installing collected packages: texttable, tensorboardX, gensim, deeprobust\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed deeprobust-0.2.4 gensim-3.8.3 tensorboardX-2.5.1 texttable-1.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from deeprobust.graph import utils\n",
        "from copy import deepcopy\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from torch_geometric.utils import from_scipy_sparse_matrix\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \"\"\" 2 Layer Graph Convolutional Network.\n",
        "    Parameters\n",
        "    ----------\n",
        "    nfeat : int\n",
        "        size of input feature dimension\n",
        "    nhid : int\n",
        "        number of hidden units\n",
        "    nclass : int\n",
        "        size of output dimension\n",
        "    dropout : float\n",
        "        dropout rate for GCN\n",
        "    lr : float\n",
        "        learning rate for GCN\n",
        "    weight_decay : float\n",
        "        weight decay coefficient (l2 normalization) for GCN. When `with_relu` is True, `weight_decay` will be set to 0.\n",
        "    with_relu : bool\n",
        "        whether to use relu activation function. If False, GCN will be linearized.\n",
        "    with_bias: bool\n",
        "        whether to include bias term in GCN weights.\n",
        "    device: str\n",
        "        'cpu' or 'cuda'.\n",
        "    Examples\n",
        "    --------\n",
        "\tWe can first load dataset and then train GCN.\n",
        "    >>> from deeprobust.graph.data import Dataset\n",
        "    >>> from deeprobust.graph.defense import GCN\n",
        "    >>> data = Dataset(root='/tmp/', name='cora')\n",
        "    >>> adj, features, labels = data.adj, data.features, data.labels\n",
        "    >>> idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "    >>> gcn = GCN(nfeat=features.shape[1],\n",
        "              nhid=16,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=0.5, device='cpu')\n",
        "    >>> gcn = gcn.to('cpu')\n",
        "    >>> gcn.fit(features, adj, labels, idx_train) # train without earlystopping\n",
        "    >>> gcn.fit(features, adj, labels, idx_train, idx_val, patience=30) # train with earlystopping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, lr=0.01, weight_decay=5e-4, with_relu=True, with_bias=True, self_loop=True ,device=None):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        assert device is not None, \"Please specify 'device'!\"\n",
        "        self.device = device\n",
        "        self.nfeat = nfeat\n",
        "        self.hidden_sizes = [nhid]\n",
        "        self.nclass = nclass\n",
        "        self.gc1 = GCNConv(nfeat, nhid, bias=with_bias,add_self_loops=self_loop)\n",
        "        self.gc2 = GCNConv(nhid, nclass, bias=with_bias,add_self_loops=self_loop)\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        if not with_relu:\n",
        "            self.weight_decay = 0\n",
        "        else:\n",
        "            self.weight_decay = weight_decay\n",
        "        self.with_relu = with_relu\n",
        "        self.with_bias = with_bias\n",
        "        self.output = None\n",
        "        self.best_model = None\n",
        "        self.best_output = None\n",
        "        self.edge_index = None\n",
        "        self.edge_weight = None\n",
        "        self.features = None\n",
        "        \n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        if self.with_relu:\n",
        "            x = F.relu(self.gc1(x, edge_index,edge_weight))\n",
        "        else:\n",
        "            x = self.gc1(x, edge_index,edge_weight)\n",
        "\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, edge_index,edge_weight)\n",
        "        return x\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize parameters of GCN.\n",
        "        \"\"\"\n",
        "        self.gc1.reset_parameters()\n",
        "        self.gc2.reset_parameters()\n",
        "\n",
        "    def fit(self, features, adj, labels, idx_train, idx_val=None, train_iters=200, initialize=True, verbose=False, **kwargs):\n",
        "        \"\"\"Train the gcn model, when idx_val is not None, pick the best model according to the validation loss.\n",
        "        Parameters\n",
        "        ----------\n",
        "        features :\n",
        "            node features\n",
        "        adj :\n",
        "            the adjacency matrix. The format could be torch.tensor or scipy matrix\n",
        "        labels :\n",
        "            node labels\n",
        "        idx_train :\n",
        "            node training indices\n",
        "        idx_val :\n",
        "            node validation indices. If not given (None), GCN training process will not adpot early stopping\n",
        "        train_iters : int\n",
        "            number of training epochs\n",
        "        initialize : bool\n",
        "            whether to initialize parameters before training\n",
        "        verbose : bool\n",
        "            whether to show verbose logs\n",
        "        normalize : bool\n",
        "            whether to normalize the input adjacency matrix.\n",
        "        patience : int\n",
        "            patience for early stopping, only valid when `idx_val` is given\n",
        "        \"\"\"\n",
        "\n",
        "        if initialize:\n",
        "            self.initialize()\n",
        "\n",
        "        self.edge_index, self.edge_weight = from_scipy_sparse_matrix(adj)\n",
        "        self.edge_index, self.edge_weight = self.edge_index.to(self.device), self.edge_weight.float().to(self.device)\n",
        "\n",
        "        if sp.issparse(features):\n",
        "            features = utils.sparse_mx_to_torch_sparse_tensor(features).to_dense().float()\n",
        "        else:\n",
        "            features = torch.FloatTensor(np.array(features))\n",
        "        self.features = features.to(self.device)\n",
        "        self.labels = torch.LongTensor(np.array(labels)).to(self.device)\n",
        "\n",
        "\n",
        "        if idx_val is None:\n",
        "            self._train_without_val(self.labels, idx_train, train_iters, verbose)\n",
        "        else:\n",
        "            self._train_with_val(self.labels, idx_train, idx_val, train_iters, verbose)\n",
        "\n",
        "    def _train_without_val(self, labels, idx_train, train_iters, verbose):\n",
        "        self.train()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        for i in range(train_iters):\n",
        "            optimizer.zero_grad()\n",
        "            output = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "            loss_train = F.cross_entropy(output[idx_train], labels[idx_train])\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            if verbose and i % 10 == 0:\n",
        "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
        "\n",
        "        self.eval()\n",
        "        output = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "        self.output = output\n",
        "\n",
        "    def _train_with_val(self, labels, idx_train, idx_val, train_iters, verbose):\n",
        "        if verbose:\n",
        "            print('=== training gcn model ===')\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        best_loss_val = 100\n",
        "        best_acc_val = 0\n",
        "\n",
        "        for i in range(train_iters):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            output = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "            loss_train = F.cross_entropy(output[idx_train], labels[idx_train])\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if verbose and i % 10 == 0:\n",
        "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
        "\n",
        "            self.eval()\n",
        "            output = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "            loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
        "            acc_val = utils.accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "            if acc_val > best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                self.output = output\n",
        "                weights = deepcopy(self.state_dict())\n",
        "\n",
        "        if verbose:\n",
        "            print('=== picking the best model according to the performance on validation ===')\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "\n",
        "    def test(self, idx_test):\n",
        "        \"\"\"Evaluate GCN performance on test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx_test :\n",
        "            node testing indices\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        output = self.forward(self.features, self.edge_index,self.edge_weight)\n",
        "        loss_test = F.cross_entropy(output[idx_test], self.labels[idx_test])\n",
        "        acc_test = utils.accuracy(output[idx_test], self.labels[idx_test])\n",
        "        print(\"Test set results:\",\n",
        "              \"loss= {:.4f}\".format(loss_test.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "        return output\n",
        "\n",
        "# %%"
      ],
      "metadata": {
        "id": "69mewhSdsMA0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from torch_geometric.utils import from_scipy_sparse_matrix\n",
        "\n",
        "class Coteaching(nn.Module):\n",
        "    \"\"\" 2 Layer Graph Convolutional Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, lr=0.01, weight_decay=5e-4,device=None):\n",
        "\n",
        "        super(Coteaching, self).__init__()\n",
        "        assert device is not None, \"Please specify 'device'!\"\n",
        "        self.device = device\n",
        "        self.nfeat = nfeat\n",
        "        self.hidden_sizes = [nhid]\n",
        "        self.nclass = nclass\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.output = None\n",
        "        self.best_model = None\n",
        "        self.edge_index = None\n",
        "        self.edge_weight = None\n",
        "        self.features = None\n",
        "\n",
        "        self.GCN1 = GCN(nfeat,nhid,nclass,dropout,device=device)\n",
        "        self.GCN2 = GCN(nfeat,nhid,nclass,dropout,device=device)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "\n",
        "        return self.GCN1(x,edge_index,edge_weight), self.GCN2(x,edge_index,edge_weight)\n",
        "\n",
        "    def fit(self, features, adj, labels, idx_train, idx_val=None, noise_rate=0.2, ek=10,train_iters=200, verbose=True):\n",
        "        \"\"\"Train the gcn model, when idx_val is not None, pick the best model according to the validation loss.\n",
        "        Parameters\n",
        "        \"\"\"\n",
        "\n",
        "        self.edge_index, self.edge_weight = from_scipy_sparse_matrix(adj)\n",
        "        self.edge_index, self.edge_weight = self.edge_index.to(self.device), self.edge_weight.float().to(self.device)\n",
        "\n",
        "        if sp.issparse(features):\n",
        "            features = sparse_mx_to_torch_sparse_tensor(features).to_dense().float()\n",
        "        else:\n",
        "            features = torch.FloatTensor(np.array(features))\n",
        "        self.features = features.to(self.device)\n",
        "        self.labels = torch.LongTensor(np.array(labels)).to(self.device)\n",
        "\n",
        "        self.noise_rate = noise_rate\n",
        "        self._train_with_val(self.labels, idx_train, idx_val, ek ,train_iters, verbose)\n",
        "\n",
        "    def _train_with_val(self, labels, idx_train, idx_val, ek,train_iters, verbose):\n",
        "        if verbose:\n",
        "            print('=== training gcn model ===')\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        best_loss_val = 100\n",
        "        best_acc_val = 0\n",
        "        idx_train = np.asarray(idx_train)\n",
        "\n",
        "        for i in range(train_iters):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            output1, output2 = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "\n",
        "            pred_1 = output1[idx_train].max(1)[1]\n",
        "            pred_2 = output2[idx_train].max(1)[1]\n",
        "\n",
        "\n",
        "            disagree = (pred_1 != pred_2).cpu().numpy()\n",
        "            idx_update = idx_train[disagree]\n",
        "\n",
        "            if len(idx_update) == 0:\n",
        "                break\n",
        "\n",
        "            k = int((1 - min(i*self.noise_rate/ek, self.noise_rate)) * len(idx_update))\n",
        "\n",
        "            loss_1 = F.cross_entropy(output1[idx_update], labels[idx_update], reduction='none')\n",
        "            loss_2 = F.cross_entropy(output2[idx_update], labels[idx_update], reduction='none')\n",
        "\n",
        "            _, topk_1 = torch.topk(loss_1, k, largest=False)\n",
        "            _, topk_2 = torch.topk(loss_2, k, largest=False)\n",
        "\n",
        "            loss_train = loss_1[topk_2].mean() + loss_2[topk_1].mean()\n",
        "\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # if verbose and i % 10 == 0:\n",
        "\n",
        "            self.eval()\n",
        "            output1, output2 = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "            acc_val = max(accuracy(output1[idx_val], labels[idx_val]),accuracy(output2[idx_val], labels[idx_val]))\n",
        "\n",
        "            if acc_val > best_acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                weights = deepcopy(self.state_dict())\n",
        "            if verbose and i % 1 == 0:\n",
        "                print('Epoch {}, training loss: {}, acc_val: {:.4f}'.format(i, loss_train.item(),acc_val))\n",
        "\n",
        "        if verbose:\n",
        "            print('=== picking the best model according to the performance on validation ===')\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "\n",
        "    def test(self, idx_test):\n",
        "        \"\"\"Evaluate GCN performance on test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx_test :\n",
        "            node testing indices\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        output1, output2 = self.forward(self.features, self.edge_index, self.edge_weight)\n",
        "        acc_1 = accuracy(output1[idx_test], self.labels[idx_test])\n",
        "        acc_2 = accuracy(output2[idx_test], self.labels[idx_test])\n",
        "        print(\"Test set results:\",\n",
        "              \"acc_1= {:.4f}\".format(acc_1.item()),\n",
        "              \"acc_2y= {:.4f}\".format(acc_2.item()))\n",
        "        return output1,output2\n",
        "\n",
        "\n",
        "# %%"
      ],
      "metadata": {
        "id": "9lTyvsbur1rh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "from deeprobust.graph.data import Dataset, PrePtbDataset\n",
        "from deeprobust.graph.utils import preprocess\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Training settings\n",
        "debug=True\n",
        "seed =11\n",
        "ek=50\n",
        "lr=0.01\n",
        "weight_decay=5e-4\n",
        "hidden =16\n",
        "dropout =0.5\n",
        "dataset ='cora'\n",
        "epochs=10\n",
        "label_rate=0.075\n",
        "ptb_rate=0.2\n",
        "noise=\"uniform\"\n",
        "cuda =torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "np.random.seed(15) # Here the random seed is to split the train/val/test data, we need to set the random seed to be the same as that when you generate the perturbed graph\n",
        "\n",
        "data = Dataset(root='/tmp/', name=dataset, setting='nettack')\n",
        "adj, features, labels = data.adj, data.features, data.labels\n",
        "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "idx_train = idx_train[:int(label_rate * adj.shape[0])]\n",
        "#%%\n",
        "\n",
        "ptb = ptb_rate\n",
        "nclass = labels.max() + 1\n",
        "train_labels = labels[idx_train]\n",
        "noise_y, P = noisify_with_P(train_labels,nclass, ptb,10, noise)\n",
        "noise_labels = labels.copy()\n",
        "noise_labels[idx_train] = noise_y\n",
        "\n",
        "noise_val_y,_ = noisify_with_P(labels[idx_val],nclass, ptb,10)\n",
        "# noise_labels[idx_val] = noise_val_y\n",
        "#%%\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model = Coteaching(nfeat=features.shape[1],\n",
        "            nhid=hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout, device=device).to(device)\n",
        "\n",
        "#%%\n",
        "model.fit(features, adj, noise_labels, idx_train, idx_val,train_iters=200, ek=ek,verbose=debug)\n",
        "model.test(idx_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tNpU0iXr_Ag",
        "outputId": "9179b83e-733e-40fa-8a24-7d5ae286360f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Selecting 1 largest connected components\n",
            "Uniform noise\n",
            "Actual noise 0.17\n",
            "Uniform noise\n",
            "Actual noise 0.19\n",
            "=== training gcn model ===\n",
            "Epoch 0, training loss: 3.8954532146453857, acc_val: 0.4137\n",
            "Epoch 1, training loss: 3.747546672821045, acc_val: 0.4739\n",
            "Epoch 2, training loss: 3.6333444118499756, acc_val: 0.6627\n",
            "Epoch 3, training loss: 3.500622272491455, acc_val: 0.7149\n",
            "Epoch 4, training loss: 3.4140708446502686, acc_val: 0.6948\n",
            "Epoch 5, training loss: 3.295103073120117, acc_val: 0.7068\n",
            "Epoch 6, training loss: 3.213716506958008, acc_val: 0.7349\n",
            "Epoch 7, training loss: 3.152909755706787, acc_val: 0.7309\n",
            "Epoch 8, training loss: 2.9714980125427246, acc_val: 0.7189\n",
            "Epoch 9, training loss: 2.9836814403533936, acc_val: 0.7309\n",
            "Epoch 10, training loss: 2.86065673828125, acc_val: 0.7550\n",
            "Epoch 11, training loss: 2.7427096366882324, acc_val: 0.7510\n",
            "Epoch 12, training loss: 2.653336524963379, acc_val: 0.7470\n",
            "Epoch 13, training loss: 2.6189966201782227, acc_val: 0.7430\n",
            "Epoch 14, training loss: 2.5517897605895996, acc_val: 0.7349\n",
            "Epoch 15, training loss: 2.4105844497680664, acc_val: 0.7309\n",
            "Epoch 16, training loss: 2.498263359069824, acc_val: 0.7390\n",
            "Epoch 17, training loss: 2.6362838745117188, acc_val: 0.7470\n",
            "Epoch 18, training loss: 2.208207368850708, acc_val: 0.7470\n",
            "Epoch 19, training loss: 2.5466785430908203, acc_val: 0.7550\n",
            "Epoch 20, training loss: 2.209266424179077, acc_val: 0.7550\n",
            "Epoch 21, training loss: 2.360111951828003, acc_val: 0.7631\n",
            "Epoch 22, training loss: 2.3925254344940186, acc_val: 0.7631\n",
            "Epoch 23, training loss: 2.2337350845336914, acc_val: 0.7590\n",
            "Epoch 24, training loss: 2.449289321899414, acc_val: 0.7470\n",
            "Epoch 25, training loss: 2.3926329612731934, acc_val: 0.7510\n",
            "Epoch 26, training loss: 2.1488406658172607, acc_val: 0.7510\n",
            "Epoch 27, training loss: 2.1074159145355225, acc_val: 0.7550\n",
            "Epoch 28, training loss: 2.0962095260620117, acc_val: 0.7510\n",
            "Epoch 29, training loss: 2.311091661453247, acc_val: 0.7671\n",
            "Epoch 30, training loss: 2.012239933013916, acc_val: 0.7550\n",
            "Epoch 31, training loss: 2.273447275161743, acc_val: 0.7470\n",
            "Epoch 32, training loss: 1.9546613693237305, acc_val: 0.7550\n",
            "Epoch 33, training loss: 2.0089831352233887, acc_val: 0.7550\n",
            "Epoch 34, training loss: 2.2468364238739014, acc_val: 0.7590\n",
            "Epoch 35, training loss: 1.7361345291137695, acc_val: 0.7590\n",
            "Epoch 36, training loss: 1.7379207611083984, acc_val: 0.7470\n",
            "Epoch 37, training loss: 2.686654567718506, acc_val: 0.7470\n",
            "Epoch 38, training loss: 2.2966065406799316, acc_val: 0.7550\n",
            "Epoch 39, training loss: 1.7144513130187988, acc_val: 0.7631\n",
            "Epoch 40, training loss: 2.11907958984375, acc_val: 0.7711\n",
            "Epoch 41, training loss: 1.9719140529632568, acc_val: 0.7671\n",
            "Epoch 42, training loss: 2.0603151321411133, acc_val: 0.7671\n",
            "Epoch 43, training loss: 1.822089672088623, acc_val: 0.7470\n",
            "Epoch 44, training loss: 2.264315605163574, acc_val: 0.7470\n",
            "Epoch 45, training loss: 1.7402843236923218, acc_val: 0.7550\n",
            "Epoch 46, training loss: 2.560591697692871, acc_val: 0.7510\n",
            "Epoch 47, training loss: 2.015997886657715, acc_val: 0.7590\n",
            "Epoch 48, training loss: 1.7621567249298096, acc_val: 0.7631\n",
            "Epoch 49, training loss: 1.6715799570083618, acc_val: 0.7510\n",
            "Epoch 50, training loss: 1.3121271133422852, acc_val: 0.7430\n",
            "Epoch 51, training loss: 1.8355830907821655, acc_val: 0.7390\n",
            "Epoch 52, training loss: 1.9376293420791626, acc_val: 0.7309\n",
            "Epoch 53, training loss: 3.6312310695648193, acc_val: 0.7349\n",
            "Epoch 54, training loss: 2.3446364402770996, acc_val: 0.7349\n",
            "Epoch 55, training loss: 2.240797758102417, acc_val: 0.7269\n",
            "Epoch 56, training loss: 1.794658899307251, acc_val: 0.7229\n",
            "Epoch 57, training loss: 1.4314050674438477, acc_val: 0.7189\n",
            "Epoch 58, training loss: 2.616408348083496, acc_val: 0.7189\n",
            "Epoch 59, training loss: 1.890195608139038, acc_val: 0.7269\n",
            "Epoch 60, training loss: 2.116574764251709, acc_val: 0.7390\n",
            "Epoch 61, training loss: 1.9166852235794067, acc_val: 0.7390\n",
            "Epoch 62, training loss: 1.8194468021392822, acc_val: 0.7430\n",
            "Epoch 63, training loss: 1.612349033355713, acc_val: 0.7430\n",
            "Epoch 64, training loss: 1.920013189315796, acc_val: 0.7430\n",
            "Epoch 65, training loss: 2.0720510482788086, acc_val: 0.7269\n",
            "Epoch 66, training loss: 1.7685933113098145, acc_val: 0.7309\n",
            "Epoch 67, training loss: 1.9519357681274414, acc_val: 0.7309\n",
            "Epoch 68, training loss: 2.1568593978881836, acc_val: 0.7229\n",
            "Epoch 69, training loss: 2.0201339721679688, acc_val: 0.7149\n",
            "Epoch 70, training loss: 2.098191022872925, acc_val: 0.7189\n",
            "Epoch 71, training loss: 1.5145198106765747, acc_val: 0.7229\n",
            "Epoch 72, training loss: 1.6529934406280518, acc_val: 0.7229\n",
            "Epoch 73, training loss: 1.4869389533996582, acc_val: 0.7309\n",
            "Epoch 74, training loss: 1.8276708126068115, acc_val: 0.7349\n",
            "Epoch 75, training loss: nan, acc_val: 0.7349\n",
            "Epoch 76, training loss: 2.063034772872925, acc_val: 0.7309\n",
            "Epoch 77, training loss: 1.6322845220565796, acc_val: 0.7309\n",
            "Epoch 78, training loss: 1.267336368560791, acc_val: 0.7430\n",
            "Epoch 79, training loss: 1.9212517738342285, acc_val: 0.7470\n",
            "Epoch 80, training loss: 1.2031946182250977, acc_val: 0.7430\n",
            "Epoch 81, training loss: 1.745449423789978, acc_val: 0.7510\n",
            "Epoch 82, training loss: 2.0611658096313477, acc_val: 0.7309\n",
            "Epoch 83, training loss: 1.9073889255523682, acc_val: 0.7269\n",
            "Epoch 84, training loss: 1.9215986728668213, acc_val: 0.7229\n",
            "Epoch 85, training loss: 1.713281512260437, acc_val: 0.7269\n",
            "Epoch 86, training loss: 2.121431589126587, acc_val: 0.7390\n",
            "Epoch 87, training loss: 2.0879921913146973, acc_val: 0.7349\n",
            "Epoch 88, training loss: 1.9231178760528564, acc_val: 0.7430\n",
            "Epoch 89, training loss: 1.3386093378067017, acc_val: 0.7390\n",
            "Epoch 90, training loss: 1.6888422966003418, acc_val: 0.7269\n",
            "Epoch 91, training loss: 2.025357961654663, acc_val: 0.7269\n",
            "Epoch 92, training loss: 1.7921968698501587, acc_val: 0.7229\n",
            "Epoch 93, training loss: nan, acc_val: 0.7189\n",
            "Epoch 94, training loss: 2.185518741607666, acc_val: 0.7149\n",
            "Epoch 95, training loss: 1.390113353729248, acc_val: 0.7149\n",
            "Epoch 96, training loss: 2.642516851425171, acc_val: 0.7229\n",
            "Epoch 97, training loss: 1.8926851749420166, acc_val: 0.7309\n",
            "Epoch 98, training loss: 1.9530346393585205, acc_val: 0.7430\n",
            "Epoch 99, training loss: 1.6671719551086426, acc_val: 0.7430\n",
            "Epoch 100, training loss: 2.0760345458984375, acc_val: 0.7309\n",
            "Epoch 101, training loss: 1.79893159866333, acc_val: 0.7349\n",
            "Epoch 102, training loss: nan, acc_val: 0.7349\n",
            "Epoch 103, training loss: 1.8549377918243408, acc_val: 0.7390\n",
            "Epoch 104, training loss: nan, acc_val: 0.7510\n",
            "Epoch 105, training loss: 1.611331582069397, acc_val: 0.7510\n",
            "Epoch 106, training loss: nan, acc_val: 0.7430\n",
            "Epoch 107, training loss: 2.30893611907959, acc_val: 0.7390\n",
            "Epoch 108, training loss: nan, acc_val: 0.7430\n",
            "Epoch 109, training loss: nan, acc_val: 0.7470\n",
            "Epoch 110, training loss: 1.687959909439087, acc_val: 0.7550\n",
            "Epoch 111, training loss: 2.1458253860473633, acc_val: 0.7430\n",
            "Epoch 112, training loss: 1.5689237117767334, acc_val: 0.7349\n",
            "Epoch 113, training loss: 2.0120182037353516, acc_val: 0.7349\n",
            "Epoch 114, training loss: 2.015317916870117, acc_val: 0.7349\n",
            "Epoch 115, training loss: 1.6016995906829834, acc_val: 0.7390\n",
            "Epoch 116, training loss: 1.9841605424880981, acc_val: 0.7349\n",
            "Epoch 117, training loss: nan, acc_val: 0.7390\n",
            "Epoch 118, training loss: 1.4877650737762451, acc_val: 0.7430\n",
            "Epoch 119, training loss: 0.9337857961654663, acc_val: 0.7349\n",
            "Epoch 120, training loss: 1.035037636756897, acc_val: 0.7390\n",
            "Epoch 121, training loss: 0.9796757102012634, acc_val: 0.7349\n",
            "Epoch 122, training loss: 1.5478882789611816, acc_val: 0.7349\n",
            "Epoch 123, training loss: 2.0280299186706543, acc_val: 0.7269\n",
            "Epoch 124, training loss: 1.9548776149749756, acc_val: 0.7229\n",
            "Epoch 125, training loss: nan, acc_val: 0.7189\n",
            "Epoch 126, training loss: nan, acc_val: 0.7068\n",
            "Epoch 127, training loss: 1.4607418775558472, acc_val: 0.7108\n",
            "Epoch 128, training loss: 1.6120283603668213, acc_val: 0.7028\n",
            "=== picking the best model according to the performance on validation ===\n",
            "Test set results: acc_1= 0.7369 acc_2y= 0.7420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.7967, -0.4195, -0.4654,  ..., -0.4965,  0.8669, -0.5495],\n",
              "         [ 0.1696, -0.9571,  1.4728,  ...,  0.1791, -2.5532, -0.9796],\n",
              "         [ 2.5607, -0.3564, -0.7410,  ..., -2.9624,  0.4035, -2.4105],\n",
              "         ...,\n",
              "         [-0.3653, -1.5567,  2.2040,  ...,  0.0911, -2.6340, -0.5804],\n",
              "         [-0.9264, -2.1131,  2.0736,  ...,  0.0784, -2.7582, -0.0514],\n",
              "         [-0.5157, -1.1181,  1.4682,  ...,  0.0947, -1.7366, -0.4395]],\n",
              "        grad_fn=<AddBackward0>),\n",
              " tensor([[-6.2013e-01, -5.7576e-01, -4.2369e-01,  ..., -4.4566e-01,\n",
              "           4.4927e-01, -6.0386e-01],\n",
              "         [-3.1677e-01,  1.3026e-01,  2.0924e+00,  ..., -2.6809e-01,\n",
              "          -3.3383e+00, -1.0043e+00],\n",
              "         [ 2.2135e+00, -3.8122e-01, -7.2990e-01,  ..., -1.4836e+00,\n",
              "          -1.1537e+00, -1.9181e+00],\n",
              "         ...,\n",
              "         [-4.9260e-01, -2.0891e-01,  2.5263e+00,  ..., -3.6303e-01,\n",
              "          -3.2027e+00, -1.1566e+00],\n",
              "         [-1.3078e+00, -1.4068e-01,  2.3174e+00,  ..., -3.4162e-01,\n",
              "          -4.0030e+00, -4.6131e-01],\n",
              "         [-3.2846e-01, -4.3826e-01,  1.6550e+00,  ...,  3.8987e-03,\n",
              "          -1.8424e+00, -8.8390e-01]], grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}